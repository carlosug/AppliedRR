---
title: 'Applied Research: Advance Statistics with R'
author: "Carlos Utrilla Guerrero"
date: "6/08/2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

## WELCOME

### Learning outcomes:
By the end of this assignment(s), you should be able to:
*Formulate different hypothesis testing

* Interpret hypothesis testing model outputs

* Calculate correlation and covariance

* Interpret pearson correlation and scatter plots

* Loop functions in R

### Essential R assignment(s) document guidelines:
In the current document you will find the following color(s) highlight(s) and format(s). Please refer to this table for legend description.

### Dataset
In this assignment(s), we will use the following dataset:
1.	Workshop Statistics_ descriptives .xlsx.


## Quick started with hyptohesis testing

Statisticians use hypothesis testing to formally check whether the hypothesis is accepted or rejected.  Hypothesis testing refers to the process of generating a clear and testable question, collecting and analyzing appropriate data, and drawing an inference that answers your question. Of course, this means that some steps come into play. Generally speaking, hypothesis testing is conducted in the following manner:


| Phase | Definition | Explanation |
|---|---|---|
| Phase 1 | State the hypotheses | Stating the null and alternative hypotheses |
| Phase 2 | Formulate the analysis Plan | The selection of the statistical procedure is crucial |
| Phase 3 | Analyze sample data | Calculation and interpretation of the test statistics |
| Phase 4 | Interpret results | Application of the decision rules |


Remember that hypothesis testing ultimately uses a p-value to weight the strength of the evidence. The p-values ranges between 0 and 1. It can be interpreted in the following way:

- A small p-values (typically ≤ 0.05) indicates strong evidence against the null hypothesis, so you reject it.
- A large p-values (>0.05) indicates weak evidence against the null hypothesis, so you fail to reject it. 

Also, recall the two type of errors that can occur from the hypothesis testing:

* Type I Error: it occurs when the researcher rejects the null hypothesis when it is in fact true. The term significance level is used to express the probability of Type I error.

* Type II Error: it occurs when the researcher accept the null hypothesis when is in fact false. The term power is used to express this probability to happen.

Also, remember the three 'basic' versions of t-test:
1. One sample t-test which tests the mean of a single group against a known mean.
2. Independent samples t-test which compares mean for two groups.
3. Paired sample t-test which compares means from the same group at different times.

As mentioned in the lecture, today we will go through the t-test 1 and 2. In the next lecture, we will deeply have a look on all of them and many others!


Before starting with using the R functions to make the hypothesis test, let's try to solve it with a sort of by 'hand' procedure:

### By hand procedure:

#### Problem: 

_Suppose the mean weight of [baby alpacas](https://en.wikipedia.org/wiki/Alpaca#:~:text=The%20alpaca%20(Vicugna%20pacos)%20is,often%20noticeably%20smaller%20than%20llamas.) found in an Cusco (Peru) last year was 15.4 kg. In a sample of 35 baby alpacas same time this year in the same town, the mean baby alpacas weight is 14.6 kg. Assume the sample standard deviation is 2.5 kg. At .05 significance level, can we reject the null hypothesis that the mean baby alpacas weight does not differ from last year?_

#### Calculation:

The null hypothesis is that μ = 15.4. We begin with computing the test statistic:

```{r}
xbar = 14.6            # sample mean 
mu0 = 15.4             # hypothesized value 
s = 2.5                # sample standard deviation 
n = 35                 # sample size 
t = (xbar−mu0)/(s/sqrt(n)) 
t                      # test statistic 
```

Here is:

`xbar`: Sample mean
`mu0`: Hypothesized value
`sig`: Standard deviation of sample
`n`: Sample size
`t`: Test statistics

We then compute the critical values at .05 significance level. You can compute the tα/2,n−p critical value in R by doing qt(1-alpha/2, n-p).

```{r}
alpha = .05 # level significance
t.half.alpha = qt(1−alpha/2, df=n−1) # critical values
c(−t.half.alpha, t.half.alpha) # the two boundaries
```

#### Answer:

*Critical boundaries*:

The test statistic -1.8931 lies between the critical values -2.0322, and 2.0322. Hence, at .05 significance level, we do not reject the null hypothesis that the mean penguin weight does not differ from last year

*p-value*:

Instead of using the critical value, we apply the pt function to compute the two-tailed p-value of the test statistic. It doubles the lower tail p-value as the sample mean is less than the hypothesized value. Since it turns out to be greater than the .05 significance level, we do not reject the null hypothesis that μ = 15.4.

```{r}
pval = 2 ∗ pt(t, df=n−1)  # lower tail 
pval                      # two−tailed p−value
```

We have seen an _hand procedure of hypothesis testing_ with the intuitive 'critical value'and 'p-value' decision making approach. Now, we are going to use R functions in order automatically calculate such statistics.

If you want to get more familiar with the hand procedure of hypothesis testing, you have a look in the following tutorials:
- [http://www.r-tutor.com/elementary-statistics/hypothesis-testing](http://www.r-tutor.com/elementary-statistics/hypothesis-testing)
- [Hypothesis Testing by Hand: A Single Sample tTest]([https://www.youtube.com/watch?v=yvHQEJnYZBY])


### Using the Student's T-test
R can handle the various versions of T-test using the `t.test()` command. This function can be used to deal with one-sample tests as well as two-sample (un-)paired tests.

Listed below are the most common arguments used in the `t.test` and their explanation:


| Argument | Explanation |
|---|---|
| `t.test(sample1,sample2)` | The basic method of applying t.test as comparing means of numeric data. |
| `var.equal = FALSE` | If the `var.equal` instruction is set to `TRUE`, the variance is considered to be equal and the standard test is carried out. If the instruction is set to `FALSE` (the default), the variance is considered unequal and the *Welch* two-sample test is carried out
| `mu = 0` | If a one-sample test is carried out, `mu` indicates the mean against which the sample should be tested |
| `alternative = “two.sided”` | It sets the alternative hypothesis. The default value for this is “two.sided” but a greater or lesser value can also be assigned. |
| `conf.level = 0.95` | It sets the confidence level of the interval (default = 0.95) (1-alpha) |
| `paired = FALSE`| If set to `TRUE`, a matched *pair* T-test is carried out |

_Note_: Use `paired = TRUE` when testing the same set of subjects using different conditions to compare if there is a different in observations due to the new conditions. T-test to ascertain if the difference is significant within the SAME TEST GROUP under DIFFERENT CONDITIONS.
Use `paired = FALSE` when comparing samples

_Additional note_: `var.test()` to compare mean different between groups should be tested: whether two samples have unequal variances. Be aware that Student's `t-test` assumes that the two populations have normal distribution with equal variances. The F test (which is what the `var.test` does) tests whether the groups have comparable variances, if that is the case we can procede with the test however if it is significant that means that the groups have become incompable allowed to compare the groups if this var.test() is not significant.

By default, the R t.test() function makes the assumption that the variances of the two groups of samples, being compared, are different. Therefore, Welch t-test is performed by default. Welch t-test is just an adaptation of t-test, and it is used when the two samples have possibly unequal variances.

The argument “var.equal=TRUE” can be used to indicate to the t.test() function that the two samples have equal variances. However you have to check this assumption before using it.


In general, Welch's t-test, or unequal variances t-test, is a two-sample location test which is used to test the hypothesis that two populations have equal means.

More info: [here](http://www.sthda.com/english/wiki/wiki.php?id_contents=7600#:~:text=Remember%20that%20independent%20t%2Dtest,%2C%20being%20compared%2C%20are%20different.)


#### One-Sample T-test

The one-sample t-test is a member of the t-test family.  All the tests in the t-test family compare differences in mean scores of continuous-level (i.e. numerical) and normally distributed data.  Unlike the independent or dependent sample t-tests (also called unpaired or paired t-test), the one-sample t-test works with only one mean score.  The one-sample t-test compares the mean of a single sample to a predetermined value to determine if the sample mean is significantly different (greater or less) than that value.

Some possible applications of the one-sample t-test include testing a sample against a predetermined or expected value, testing a sample against a certain benchmark, or testing the results of a replicated experiment against the original study.  For example, a researcher may want to determine if the average age of retiring in a certain population is 65.  The researcher would draw a representative sample of people entering retirement and ask at what age they retired.  A one-sample t-test could then be conducted to compare the mean age obtained in the sample (e.g., 63) to the hypothetical test value of 65.  The t-test determines whether the difference we find in our sample is larger than we would expect to see by chance.

In the lecture, we conducted a experiment in order to investigate weather or not 'coffee lovers' shops pounder 33 cl on the orange juice. In our experiment we collected data of 10 orange juices and measured amount of orange cl per glass. The results are here:

| id | cl |
|---|----|
| 1 | 28 |
| 2 | 31 |
| 3 | 28 |
| 4 | 37 |
| 5 | 30 |
| 6 | 33 |
| 7 | 25 |
| 8 | 33 |
| 9 | 24 |
| 10 | 30 |

Let's create the hypothesis null for 'two-sides' in R as following:

```{r}
# Define orange sample
orange_maastricht <- c(28,31,28,37,30,
            33,25,33,24,30)

orange_amsterdam <-  c(24,31,25,37,30,
            33,23,32,24,30)

# Conduct independent t-test
t.test(x = orange_maastricht,   # sample values Maastricht
       y = orange_amsterdam,    # sample values Amsterdam
       paired = FALSE, # different and independent observations from samples
       var.equal = TRUE) # we assume variance are equal thus, we run Student t.test

```

*Model output description*


variable         | description
---------------- | ---------------------------------------------
`title`          | Type of test performed.
`t`              | t statistic.
`df`             | degrees of freedom --- sample size - 1.
`p-value`        | probability of selecting a sample with mean not equal to 33 cl.
`Ha`             | Alternative hypothesis
`C.I 95%`        | indicates that 95% of all hpothetical samples one could drawn a conclusion.
`upper-lower`    | this boudaries give and indication of most likely values.


*Model output interpretation*

From the model output, we can see that the mean orange volume in cl for the sample is 29.9 cl. The two-sided 95% confiance interval tells you that the mean orange volume is likely to be between 27.11 and 32.6 cl. The p-value of 0.033 tells you if the mean volume of orange juice were 33 cl, the probability to select a sample with a mean volume not equal to 33 cl would be approximately 3% (p-value = 0.033). Since the p-value is less than the significance level (0.05), we can reject the true mean is equal to 33 cl. This means that there is not statistical evidence that orange juice are being equal to 33 cl.

*NB*: IT IS INCORRECT TO SAY THAT YOU ARE 95% SURE THAT THIS IS THE POPULATION VALUE because the population value is 'fixed'. It is either in or outside the interval. The confidence interval might have 'caught' the value or not, 95% of the samples from the same population will contain the true value. You only don't know whether this is one of the 95%.

So you use this interval to provide an indication of potential values: if this range is very narrow, then you can be relatively sure about the value you got from the sample, wheareas if this is a wide interval you know that you cannot really trust your results. if the interval contains the value of the null hypothesis (in this case 33) that means that 33 is a likely value to happen, and that you therefore do not have a significant outcome.

#### Using Directional Hypothesis (one-sided)

In the previous experiment (orange), we are simply interested in testing if the means are different. In many other cases, however, you might want to know if the orange sample is lower or greater than 33 cl. We then use the `alternative` equal to argument to switch from `two-sided` (default) to `one-sided`.

The choices you have are between ″two.sided″, ″less″, or ″greater″. 
Suppose we want to use a one-sample t-test to determine whether the amount orange juice in coffee lovers are less than they say (33 cl). One sided test is suitable because we are specifically interested in knowing whether the volume is less than 33 cl. The test has the null hypothesis that the mean orange volume is equal to 33 cl, and the alternative hypothesis that the mean orange volume is less than 33 cl. A significant level of 0.05 is to be used.

```{r}
t.test(orange_maastricht,
       mu = 33,
       alternative = 'less') # the Ha is less than 33 cl
```
From the output we can see that the mean orange volume from the mean is 29.9 cl. The one-sided 99% confidence interval tell us that the mean orange volume is likely to be less than 33.37 cl.  The p-value of 0.01656 tell us that if the mean volume of the orange juice were 33 cl, the probability of selecting a sample with a mean less than or equal to this one would be approximately 1.6%.
Since the value is  less than the significant level 0.05, we can reject the null hypothesis that the mean orange volume is equal to 33 cl.

We could also make the same hypothesis test but with a different significant level. For instance, lets do peform the same test but with a significant level of 0.01:

```{r}
t.test(orange_maastricht,
       mu = 33,
       alternative = 'less', # upper tail test
       conf.level = 0.99)  # conf level is 1 - significant level [alpha]
```

A brief introduction to one-sample t test in R is given in the following links. Use this source in order to get more familiar with this `t.test` and the different applications in R:
+ [Performing a one-sample t-test in R](http://www.instantr.com/2012/12/29/performing-a-one-sample-t-test-in-r/)
+ [One-Sample t Test & Confidence Interval in R](https://www.youtube.com/watch?v=kvmSAXhX9Hs)

#### Exercise 1: Based on this model outputs, can you please interpret the model results and make a conclusion?



#### Exercise 2: Using `t.test`, can you please test whether the mean orange sample is greater than 33 cl? The significant level to be used is 0.05.

```{r}
t.test(orange_maastricht,
       mu = 33,
       alternative = 'greater')

```


So far we have seen how to carry out the t-test on one vector of values. However we can use specific columns from a dataset.

### Example with data

In this section, we will use the already known `descriptive` dataset:
You can download here:
Let use variable `age` from `mydat` sample from a normal distribution population with unknown mean and standard deviation. 

You can download the data here: https://docs.google.com/spreadsheets/d/1JchKI5u-I5IcWbnM_JpMkMJIZX1012oJVoIEZz2ra3c/edit#gid=0

```{r}
library(readxl)
mydat <- read_excel("../data/Workshop Statistics_ descriptives .xlsx")


mydat_original <- mydat
names(mydat)
names(mydat)[6] <-"favPet"

names(mydat)[7:8] <- c('GoT','LotR')
mydat[mydat == -99] <- NA
mydat$favPet[mydat$favPet==1] <- 0
mydat$favPet[mydat$favPet==2] <- 1
mydat$favPet <- factor(mydat$favPet, labels = c('cat','dog'))
mean(mydat$age, na.rm = TRUE)
```

As you might remember, you could calculate the mean of the variable `age` as following:

```{r}
mean(mydat$age)
```

Since we had missing data (e.g. -99), this mean is not truly representative and we have to make the  the neccessary steps to clean the data:

```{r}
# change the name cols
names(mydat)[6] <- "favPet"
names(mydat)[7:8] <- c('GoT','LotR')
# missing data
mydat[mydat == -99] <- NA
# dummy variable
mydat$favPet[mydat$favPet==1] <- 0
mydat$favPet[mydat$favPet==2] <- 1
# convert to factor
mydat$favPet <- factor(mydat$favPet, labels = c('cat','dog'))
```

Now, we can again compute the mean:

```{r}
mean(mydat$age, na.rm = TRUE)
```


Now, we want to test whether or not the mean age of the population (e.g. all students from venlo campus) is equal to 20, at significance level of 5%.

```{r}
t.test(mydat$age, #sample as vector
       mu=20) # true mean is equal to 20 age

```

*Model output interpretation*

p- value (0.00003927) is notoriously smaller than the significance level (=0.05) thus, we do have strong evidence to reject HO and we could draw a conclusion that true mean is equal NOT to 20. In addition, value of MU = 20 is not within the interval values of CI, thereof it can be rejected the Ho that true mean is equal to 20.


#### Exercise 3: Using `t.test`, can you please test whether the mean length of the true population is greater than 168 cl? The significant level to be used is 0.01.

```{r}
t.test(mydat$length, # sample of length
       mu = 168, # true mean in cm
       alternative = 'greater', # ha as true mean greater than 168 cm
       conf.level = 0.99) # significant level of 0.01
```


### (Unparied) Independent samples t-test
 
The independent sample t-test compares the mean of one distinct group to the mean of another group.  An example research question for an independent sample t-test would be, “Do boys and girls differ in their age expectancy?” 

Independent t-test or (unpaired t-test) is used to compare the means of two unrelated groups of samples. The aim of this section is to show you how to calculate independent samples t test with R software. The t-test formula is described [here](http://www.sthda.com/english/wiki/t-test-formula).


#### Example with data

The question is to test whether group 1 student's average age is significantly different from group 1 student's average age. The number of individuals considered here is obviously low. This is just to illustrate the usage of two-sample t-test.

Let's first create a summary of the mean and standard deviation for each group:

```{r}
library(dplyr) # import dplyr
summarise(group_by(mydat, group),
          count = n(),
          mean = mean(age, na.rm = TRUE))


class(mydat$group)
```

Since we got more than 2 categories within the `group` variable, the unpaired t-test can be performed as follow:

```{r}

mydat$age[mydat$group == 1]
t.test(x = mydat$age[mydat$group == 1], y = mydat$age[mydat$group == 2], # sample 1 and sample 2
       paired = FALSE, # different and independent observations from samples
       var.equal = TRUE) # we assume variance are equal thus, we run Student t.test



t.test(x = mydat$age[mydat$group == 1], y = mydat$age[mydat$group == 2], # sample 1 and sample 2
       paired = FALSE, # different and independent observations from samples
       var.equal = TRUE) # we assume variance are equal
```


In the result above:

+ `t` is the t-test statistic value (t = -0.16052)
+ `df` is the degrees of freedom (df= 17)
+ `p-value` is the significance level of the t-test (p-value = 0.8744).
+ `conf.int` is the confidence interval of the mean at 95% (conf.int = [-4.400235, 3.778013])
+ `sample estimates` is the mean value of the sample (mean = 21.88889, 22.20000).


*Model Output Interpretation*

The t-test is determining whether or not there are different between means in group 1 and 2 observation. _H0_ is there is not different between true means (mean.g1 = mean.g2) against a _Ha_ in which we assume true differences between group 1 and 2 means is not equal to 0 (mean.g1 <> mean.g2). Since p-value is greater than significance value 5%, it indicates that we can accept('fail to reject') the _H0_ of equal means. Based on this data, we conclude that there is not enough evidence of a difference between means of the group 1 and group 2. This fact it's confirmed by having 0 value within the conf.int boundaries.


#### Exercise 4: We are interested in determining whether or not the mean length of group 3 is equal or greater than mean length of group 1 respondent at significance level of 5%. Use the `t.test` to perform a two-sample tests assuming that the variances are equal.

```{r}
t.test(x = mydat$length[mydat$group == 3], y = mydat$length[mydat$group == 1], # samples filter
       alternative = 'greater', # case Ha different is greater than
       paired = FALSE, # different and independent observations from samples
       var.equal = TRUE) # we assume variance are equal thus, we run Student t.test
```


If we run the same t-test but with different variable that has only two categories, for instance, favPet ('dog','cat'), the following command in R will work:

```{r}
t.test(age ~ year, # comparing mean differences between favPet respondents
       data = mydat, # dataset
       var.equal = TRUE) # assumption variances equal between two samples
```


#### Exercise 5: Can you please compare the length means between respondents who like dog and cats as a pets? Use a significant level of 0.01 to perform a two-sample tests assuming that the variances are equal.What conclusion can be drawn from the p-value?


```{r}
t.test(length ~ favPet,
       data = mydat,
       conf.level = 0.99,
       var.equal = TRUE)
```


## Scatter plot, correlation and covariance

In statistics, you deal with a lot of data. The hard part is finding patterns that fit the data. To look for patterns, there are several statistical tools that help identify these patterns. But before you use any of these tools, you should look for basic patterns. As you learned, you can identify basic patterns using a scatter plot and correlation.

We have learned that the most useful graph for displaying the relationship between two
numerical variables is a scatterplot. As a reminder, a scatterplot shows the relationship between two numerical variables measured for the same individuals. The values of one variable appear on the horizontal axis, and the values of the other variable appear on the vertical axis. Each individual in the data appears as a point on the graph.


A correlation is a mathematical relationship between two variables. There are three ways that data can correlate: positive, negative, and zero.

Positive correlation is when the scatter plot takes a generally upward trend.It also means that the line of best fit has a positive slope.

Negative correlations, you guessed it, have a generally downward trend in the scatter plot.This means that the slope of the line of best fit has a negative slope.

Zero correlation is also referred to as no correlation. This means that the pattern has no discernible pattern. This usually implies that the two variables are unrelated. In this case, the value of the slope of the line of best fit would be very close to zero. Your previous plot is an example of zero correlation.

Let's create a simple scatter plot:

```{r}
plot(mydat$age,mydat$length)
```
According to this figure, we might conclude that there is a weak association between age and length variables. Also, we cannot clearly define the direction of this association. Relying on the interpretation of a scatterplot is too subjective. More precise evidence is needed, and this evidence is obtained by computing a coefficient that measures the strength of the relationship under investigation.



### Calculate coefficient correlation

Now, I been using the word ‘slope’ to refer to the line of best fit, but that does not really tell you the strength of the correlation. To determine the strength of the correlation, the correlation coefficient is best.
As shown in the lecture, the correlation coefficient is comprised between -1 and 1:
-1 indicates a strong negative correlation : this means that every time age increases, length decreases
0 means that there is no association between the two variables (age and length)
+1 indicates a strong positive correlation : this means that length increases with age

According with this [rule of thumb](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3576830/#:~:text=The%20Pearson's%20correlation%20coefficient%20for%20these%20variables%20is%200.80.&text=For%20example%2C%20a%20correlation%20coefficient,use%20the%20most%20appropriate%20one.) for interpreting the size of the correlation coefficient, we could consider that there are not a clearly association between both variables


Simple correlations are between two continuous variables and we use the `cor()` R function to obtain a correlation coefficient, as shown in the following command:


```{r}
# correlation between age and length
cor(mydat$age, mydat$length)
```

It returns NA. As it happens with other descriptive statistics (e.g. mean, variance and standard deviation), the function will return NA in case there are existing missing values. Inconveniently, the developers of the cov function did not use the na.rm = TRUE, but allow you to chose different options for what you want to do with the missings. In general you want to use listwise deletion or complete cases only.The problem with pair wise deletion is that you get biased parameter estimtes
# hence we use 'complete.obs' for the cov. Then, we add the `use` argument in our R code:

```{r}
cor(mydat$age, mydat$length, 
    use= 'complete.obs') # deal with NA
```


We could also consider to compute the correlation applying other methods. The default value of the method argument is `pearson`, which it is why we didn’t have to specify it in the earlier R code. For instance, we can calculate the correlation using `spearman` methods like this:



```{r}
mydat$year <- factor(mydat$year)
class(mydat$year)
levels(mydat$year)
```




Broadly speaking, you can use `Spearman` method when you have ranked (ordinal) variables. These variables are those for which the individual observations can be put in order from smallest to largest. You can have a nice read here: http://www.biostathandbook.com/spearman.html

### Calculate covariance

The `cov()` command uses syntax similar to the `cor()` command to examine covariance.

Can you please check the relationship between variable `age` and `leng`. The argument `use` is applied to deal with N/A values. You can use the `help` function to see exactly what this default argument is about.


```{r}
cov(mydat$age, mydat$length, use = 'complete.obs')
```
The covariance is 6.189 an indication of a positive linear relationship between two variables.

*What are the differenes between covariance and correlation?*
In simple words, both the terms (correlation and covariance) measure the relationship and the dependency between two variables. “Covariance” indicates the direction of the linear relationship between variables. “Correlation” on the other hand measures both the strength and direction of the linear relationship between two variables. Correlation is a function of the covariance. What sets them apart is the fact that correlation values are standardized whereas, covariance values are not. You can obtain the correlation coefficient of two variables by dividing the covariance of these variables by the product of the standard deviations of the same values.

Check this article to get more insight about its differences: https://towardsdatascience.com/let-us-understand-the-correlation-matrix-and-covariance-matrix-d42e6b643c22#:~:text=In%20simple%20words%2C%20both%20the,linear%20relationship%20between%20two%20variables.


### Correlation test

Similar to hypothesis test, the association between two numerical variables can be evaluated using the `cor.test` command. This R function test for association/correlation between samples. It returns both the correlation coefficient and the significance level(or p-value) of the correlation .

The simplified format of `cor.test` is:

cor.test(x, y, method=c("pearson", "kendall", "spearman"))


where:

+ x, y: numeric vectors with the same length
+ method: type of correlation method

Let's create correlation test between `age` and `length` variables:

```{r}
cor.test(mydat$age, mydat$length)
cor(mydat$age, mydat$length, use = "complete.obs")
hist(mydat$length)
```

*Model Output Description*

t: is the t-test statistic value (t = 2.1691)
df: is the degrees of freedom (df= 65),
p-value: is the significance level of the t-test (p-value = 0.03374).
conf.int: is the confidence interval of the correlation coefficient at 95% (conf.int = [0.02089861, 0.47064042]);
cor: sample estimates is the correlation coefficient (Cor.coeff = -0.259803)


*Model Output Interpretation*

The p-value of the test is 0.03374, which is less than the significance level alpha = 0.05. We can conclude that age and length are not significantly correlated. This conclusion is supported by the slightly moderate correlation coefficient of -0.259. 


### Visualise correlation

In order to create a meaninful scatter plot for correlation analysis, we could consider to add into the graph a line that represent the direction of the relationship. For this purpose, we can use this command and run it together (select both code lines and run):


```{r}
plot(mydat$age,mydat$length)
abline(lm(length~age,mydat))

```

You could see from the scatter plot that including this line, add more information about the slope/direction of the relationship.

Let's create together a much more informative scatter plot that can be undoubtly used in a scientific report with the following inputs:
- Line representing the relationship between variables (linear regression)
- Shape of the confidence intervals for regression line
- r as coefficient value
- p value to make conclusion


```{r}
library(ggpubr)
ggscatter(mydat, x = 'age', y = 'length', # sample data
          add = 'reg.line', # add regression line
          conf.int = TRUE, # add its conf interval
           add.params = list(color = "blue",
                            fill = "lightgray"),
          cor.coef = TRUE, # add r coef
          cor.method = 'pearson',
          xlab = "age", ylab = "length(cm)")
```


### GOING FORWARD

You’ve covered a lot of ground in this tutorial, so congratulations for making it through to the end!

I hope you are able to put these concepts to work in your own analytical adventures! These are really the basics of data exploration and inferential statistics in R and there’s so much more that you can do to make sure that you get a good feel for your data before you start analyzing and modeling it. 

In the mean time, I encourage you to head on over to Kaggle and find a rich, fascinating data set of your own to explore if you don’t want to continue with the `trial_ACTG175.xls` data set!

## Bonus section: Standard deviation, Standard Error and Loop function

1.  What is the age mean within each group and over all groups? 

```{r}
# Step 1: Compute the mean and standard deviation of the entire dataset (= over all groups)
mydat.original <- mydat
paste0( 'The mean of the age for the entire data set is ', mean(mydat$age, na.rm = TRUE))
```

```{r}
# Step 2: compute the mean within each group (=col 1 and has seven categories)

# create two objects in which the mean (or length) is stored

ageGrouped <- rep(NA, times =7) 
lengthGrouped <- rep(NA, times =7)

# For each group, compute the age mean, allocate the information into the desired object
ageGrouped[1] <- mean(mydat$age[mydat$group ==1], na.rm = TRUE) 
ageGrouped[2] <- mean(mydat$age[mydat$group ==2], na.rm = TRUE)
ageGrouped[3] <- mean(mydat$age[mydat$group==3], na.rm = TRUE)
ageGrouped[4] <- mean(mydat$age[mydat$group==4], na.rm = TRUE)
ageGrouped[5] <- mean(mydat$age[mydat$group==5], na.rm = TRUE)
ageGrouped[6] <- mean(mydat$age[mydat$group==6], na.rm = TRUE)
ageGrouped[7] <- mean(mydat$age[mydat$group==7], na.rm = TRUE)

# Repeat same logic to compute length mean
lengthGrouped[1] <- mean(mydat$length[mydat$group ==1], na.rm = TRUE) 
lengthGrouped[2] <- mean(mydat$length[mydat$group ==2], na.rm = TRUE)
lengthGrouped[3] <- mean(mydat$length[mydat$group==3], na.rm = TRUE)
lengthGrouped[4] <- mean(mydat$length[mydat$group==4], na.rm = TRUE)
lengthGrouped[5] <- mean(mydat$length[mydat$group==5], na.rm = TRUE)
lengthGrouped[6] <- mean(mydat$length[mydat$group==6], na.rm = TRUE)
lengthGrouped[7] <- mean(mydat$length[mydat$group==7], na.rm = TRUE)
```

***Remark: We are doing essentially a filter of our data set. In the script we tell to R to allocate in the new object, the results of the mean for the entire dataset, select age and filter per group***

## Parenthesis:

Now if this feels tidious to you, you are absolutely right! there is a much quicker way to do it, 

Like build our loop function, which will save you countless hours by automating your mean calculation. 

You can use what ever letter you like here as long as you keep using the same one the i in 1: 7 will give i first the value 1, execute everthing between the curly brackets, then i becomes 2 etc. until 7 

**You can compare the old and new objects and see they are identical,but now it with only 4 lines of code!**

```{r}
# first create a two new objects in which the results of loop function will be stored
ageGroupedWithForLoop <- rep(NA, times =7)
lengthGroupedWithForLoop <- rep(NA, times =7)

for(i in 1:7){
  ageGroupedWithForLoop[i] <- mean(mydat$age[mydat$group == i], na.rm = TRUE)
  lengthGroupedWithForLoop[i] <- mean(mydat$length[mydat$group == i], na.rm = TRUE)
}

# Compare means for age
mean(mydat$age)
mean(ageGrouped, na.rm = TRUE)
mean(ageGroupedWithForLoop, na.rm = TRUE)

# compare means for length
sd(mydat$length)
sd(lengthGrouped, na.rm = TRUE)
sd(lengthGroupedWithForLoop, na.rm = TRUE)
```


2.  Now, it is time to replicate previous steps to compute the standard deviation. Can you make it with For Loop method?

```{r}
paste0('The standard deviation of age for the entire data set is ', round(sd(mydat$age),digits = 2))
paste0('The standard deviation of length for the entire data set is ', sd(mydat$length))
```


```{r}
# Create the new object that will store result of standard deviation for age and length
SDageGroupedwithForLoop <- rep(NA, times = 7)
SDlengthGroupedwithForLoop <- rep(NA, times = 7)

# Create for loop function

for (i in 1:7) {
  SDageGroupedwithForLoop[i] <- sd(mydat$age[mydat$group == i], na.rm = TRUE)
  SDlengthGroupedwithForLoop[i] <- sd(mydat$length[mydat$group == i], na.rm = TRUE)
}

# Compare standard deviations
sd(mydat$age)
sd(ageGrouped, na.rm = TRUE)
sd(ageGroupedWithForLoop, na.rm = TRUE)

```

## What do you notice? 

The standard deviation over the means (`sd(ageGroupedWithForLoop, na.rm = TRUE)`) is smaller than the standard deviation in each of the groups (elements on `SDageGroupedWithForLoop()` object. 



## APPLIED YOUR KNOWLEDGES

### Research Question: Is there a correlation between a patient’s age and their CD4 T cell count at baseline?

*1. Create a plot showing the relationship between age and CD4 T cell count at baseline. Based on this plot, do you expect to find a relationship between the two variables?*

*2. Using the cor.test() function which conducts a correlation test between a patient’s age at baseline (age) and their CD4 T cell count at baseline (cd4_baseline).*

### Research Question: Do patients with a history of intravenous drug use have higher CD4 T cell counts at baseline compared to patients without a history of intravenous drug use?

*1.  create a plot showing the distribution of CD4 T cell counts at baseline (cd4_baseline) depending on whether or not patients were symptomatic (symptomatic variable). Based on this plot, do you expect to find that symptomatic patients generally have higher (or lower) CD4 T cell counts?*

*2.Use t.test() function to conduct a two-sample t-test comparing patients’ CD4 T cell count at baseline (cd4_baseline) between patients that are symptomatic and those that are not (symptomatic)*
